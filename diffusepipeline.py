# -*- coding: utf-8 -*-
"""DiffusePipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ksDLLjNq8I95lDt8MWlPHe8oGZkdqRTX

## Looking inside the pipeline
"""

!pip install transformers

# from transformers import CLIPTextModel, CLIPTokenizer
# import torch
!pip install diffusers

tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")

from diffusers import AutoencoderKL, UNet2DConditionModel
vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-ema", torch_dtype=torch.float16).to("cuda")
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")

import matplotlib.pyplot as plt
beta_start,beta_end = 0.00085,0.012
plt.plot(torch.linspace(beta_start**0.5, beta_end**0.5, 1000) ** 2)
plt.xlabel('Timestep')
plt.ylabel('Î²');

from diffusers import LMSDiscreteScheduler
scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule="scaled_linear", num_train_timesteps=1000)

prompt = ["a photograph of an astronaut riding a horse"]
height = 512
width = 512
num_inference_steps = 70
guidance_scale = 7.5
batch_size = 1

text_input = tokenizer(prompt, padding='max_length',max_length = tokenizer.model_max_length,truncation = True)
text_input

tokenizer.decode(10668)

text_embeddings = text_encoder(torch.tensor(text_input.input_ids).to("cuda"))[0].half()
text_embeddings.shape

text_embeddings

max_length = torch.tensor(text_input.input_ids).shape[-1]
uncond_input = tokenizer([""] * batch_size, padding = 'max_length',max_length =max_length,return_tensors = 'pt')
uncond_embeddings = text_encoder(torch.tensor(uncond_input.input_ids).to('cuda'))[0].half()
uncond_embeddings.shape

text_embeddings = torch.cat([uncond_embeddings,text_embeddings])

torch.manual_seed(100)
latents = torch.randn((batch_size,unet.in_channels,height //8, width // 8))
latents = latents.to('cuda').half()
latents.shape

scheduler.set_timesteps(num_inference_steps)

latents = latents * scheduler.init_noise_sigma

scheduler.timesteps

scheduler.sigmas

plt.plot(scheduler.timesteps, scheduler.sigmas[:-1]);

from tqdm.auto import tqdm

for i,t in enumerate(tqdm(scheduler.timesteps)):
  input = torch.cat([latents]*2)
  input = scheduler.scale_model_input(input,t)

  # predict the noise residual
  with torch.no_grad():
    pred = unet(input,t,encoder_hidden_states = text_embeddings).sample

  # perform guidance
  pred_uncond,pred_text = pred.chunk(2)
  pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)

  # compute the 'prev' noisy sample
  latents = scheduler.step(pred,t,latents).prev_sample

with torch.no_grad(): image = vae.decode(1/0.18215 * latents).sample

from PIL import Image
image = (image / 2 + 0.5).clamp(0, 1)
image = image[0].detach().cpu().permute(1,2,0).numpy()
image = (image * 255).round().astype('uint8')
Image.fromarray(image)

prompts = [
    'a photograph of an astronaut riding a horse',
    'an oil painting of an astronaut riding a horse in the style of grant wood'
]

text_input = tokenizer(prompts, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")
text_embeddings = text_encoder(text_input.input_ids.to("cuda"))[0].half()

max_length = text_input.input_ids.shape[-1]
uncond_input = tokenizer([""] * len(prompts), padding="max_length", max_length=max_length, return_tensors="pt")
uncond_embeddings = text_encoder(uncond_input.input_ids.to("cuda"))[0].half()
emb = torch.cat([uncond_embeddings, text_embeddings])

torch.manual_seed(100)
g = guidance_scale

latents = torch.randn((len(prompts), unet.in_channels, height//8, width//8))
scheduler.set_timesteps(num_inference_steps)
latents = latents.to("cuda").half() * scheduler.init_noise_sigma

for i,ts in enumerate(tqdm(scheduler.timesteps)):
    inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)
    with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)
    pred = u + g*(t-u)
    latents = scheduler.step(pred, ts, latents).prev_sample

with torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample
res = (image / 2 + 0.5).clamp(0, 1)

image = res[0].detach().cpu().permute(1, 2, 0).numpy()
image = (image * 255).round().astype("uint8")
Image.fromarray(image)

image = res[1].detach().cpu().permute(1, 2, 0).numpy()
image = (image * 255).round().astype("uint8")
Image.fromarray(image)

def text_enc(prompts, maxlen=None):
    if maxlen is None: maxlen = tokenizer.model_max_length
    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt")
    return text_encoder(inp.input_ids.to("cuda"))[0].half()

def mk_img(t):
    image = (t/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()
    return Image.fromarray((image*255).round().astype("uint8"))

def mk_samples(prompts, g=7.5, seed=100, steps=70):
    bs = len(prompts)
    text = text_enc(prompts)
    uncond = text_enc([""] * bs, text.shape[1])
    emb = torch.cat([uncond, text])
    if seed: torch.manual_seed(seed)

    latents = torch.randn((bs, unet.in_channels, height//8, width//8))
    scheduler.set_timesteps(steps)
    latents = latents.to("cuda").half() * scheduler.init_noise_sigma

    for i,ts in enumerate(tqdm(scheduler.timesteps)):
        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)
        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)
        pred = u + g*(t-u)
        latents = scheduler.step(pred, ts, latents).prev_sample

    with torch.no_grad(): return vae.decode(1 / 0.18215 * latents).sample

prompts = ["cube cutout of an isometric programmer's bedroom, 3d art, professional colors, soft lighting, high detail, artstation, concept art, behance, ray tracing",
           "Portrait of an owl, steampunk, indigo blue, colorful, illustration, highly detailed, simple, smooth, and clean vector, no jagged lines, vector art, smooth, made all with grey colored gears inspired by future technology"]
images = mk_samples(prompts)

from IPython.display import display
for img in images: display(mk_img(img))

